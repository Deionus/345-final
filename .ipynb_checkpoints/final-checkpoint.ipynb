{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification for Movie Reviews\n",
    "\n",
    "## Goal\n",
    "\n",
    "With this project, we aimed to use the machine learning skills in this class and apply it to a popular field in machine learning: text classification. We found a great dataset that put together 50,000 movie reviews for binary classification based on whether the movie review was positive or negative. This is useful for websites that serve as aggregates for movie reviews, such as Rotten tomatoes, as users can submit their reviews. \n",
    "\n",
    "## Step 1: Feature Exploration\n",
    "\n",
    "The dataset divided the reviews into 25,000 positive reviews and 25,000 negative reviews split evenly into training and test sets. Furthermore, the creators of the dataset turned each review into a tokenized bag of words that matches with a vocab set. The vocab set list has every words that comes up in the review set and maps it to a number. However, there are still a few things we needed to do to get the data ready for classification.\n",
    "\n",
    "## Step 2: Text Clean-Up and Transformation\n",
    "\n",
    "While the bag of words is a great start, the word count doesn't give the whole story. We want by representing not only the word count in the review but how relative it is in document frequency (A word that appears in every document is not as useful as a word that only appears in positive movies for example. We will achieve this by using a tf-idf (term frequency to inverse document frequency) score which we will then vectorize to use for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Finding the frequency of each term\n",
    "\n",
    "vocab = open(\"aclImdb/imdb.vocab\", errors='ignore').read().splitlines()\n",
    "trainReviews = open(\"aclImdb/train/labeledBow.feat\").read().splitlines()\n",
    "testReviews = open(\"aclImdb/test/labeledBow.feat\").read().splitlines()\n",
    "docFreqTrain = [0] * len(vocab)\n",
    "\n",
    "for line in trainReviews:\n",
    "    bag = line.split()\n",
    "    bag.pop(0)\n",
    "    for word in bag:\n",
    "        pair = word.split(\":\")\n",
    "        docFreqTrain[int(pair[0])] += 1 \n",
    "\n",
    "# Calculating tf-idf score for standardizing\n",
    "\n",
    "def TF_score(pair):\n",
    "    left = int(pair[0])\n",
    "    right = int(pair[1])\n",
    "    return right * math.log(25000/int(docFreqTrain[left]))\n",
    "\n",
    "def getFile(reviews, score):\n",
    "    updatedReviews = \"\"\n",
    "    for line in reviews:\n",
    "        bag = line.split()\n",
    "        updateLine = \"0 \" if (int(bag.pop(0)) <= 4) else \"1 \"\n",
    "        for word in bag:\n",
    "            pair = word.split(\":\")\n",
    "            updateLine += pair[0]\n",
    "            updateLine += \":\"\n",
    "            updateLine += str(score(pair))\n",
    "            updateLine += \" \"\n",
    "        updateLine += \"\\n\"\n",
    "        updatedReviews += updateLine\n",
    "    return updatedReviews\n",
    "\n",
    "updatedReviewsTrain = getFile(trainReviews, TF_score_Train)\n",
    "updatedReviewsTest = getFile(testReviews, TF_score_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will vectorize and turn the data into an array. For now, we will treat each word as a feature, but this will change when we optimize for hyper paramaters and classification choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 89527) (25000,)\n",
      "(25000, 89527) (25000,)\n"
     ]
    }
   ],
   "source": [
    "TrainData = updatedReviewsTrain.splitlines()\n",
    "TestData = updatedReviewsTest.splitlines()\n",
    "\n",
    "TrainFeatures = np.zeros((len(TrainData), len(vocab)))\n",
    "TrainLabels = np.zeros(len(TrainData))\n",
    "TestFeatures = np.zeros((len(TestData), len(vocab)))\n",
    "TestLabels = np.zeros(len(TestData))\n",
    "\n",
    "for i in range(0, len(TrainData)):\n",
    "    bag = TrainData[i].split()\n",
    "    TrainLabels[i] = int(bag.pop(0))\n",
    "    for word in bag:\n",
    "        pair = word.split(\":\")\n",
    "        TrainFeatures[i][int(pair[0])] = float(pair[1])\n",
    "\n",
    "for i in range(0, len(TestData)):\n",
    "    bag = TestData[i].split()\n",
    "    TestLabels[i] = int(bag.pop(0))\n",
    "    for word in bag:\n",
    "        pair = word.split(\":\")\n",
    "        TestFeatures[i][int(pair[0])] = float(pair[1])\n",
    "        \n",
    "print(TrainFeatures.shape, TrainLabels.shape)\n",
    "print(TestFeatures.shape, TestLabels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just to see how the data perform without any optimization, we'll run it through a basic svm classifier and see how the accuracy is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/miniconda3/envs/minimal_ds/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Set:  1.0 \n",
      "\n",
      "Accuracy of Test Set:  0.84776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(TrainFeatures, TrainLabels)\n",
    "\n",
    "print(\"Accuracy of Train Set: \", svc.score(TrainFeatures, TrainLabels), \"\\n\")\n",
    "print(\"Accuracy of Test Set: \", svc.score(TestFeatures, TestLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier has decent accuracy on the test, but as we can see when compared to the acurracy of the Train Set, we may have some overfitting that is not well generalized for test data. We also need to do a more in depth process for classification to figure out which classifier is the best overall. This will be covered in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Further Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZER\n",
    "\n",
    "This class takes in the File Name of a movie review and converts it into a Bag of Words using the imdb.vocab file.\n",
    "\n",
    "RETURNS a string with 0 at the front and a count of each word present afterwards, delimited by whitespace. No newline character at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def Tokenizer(file_name):\n",
    "    vocab = open(\"aclImdb/imdb.vocab\").read().splitlines()\n",
    "    \n",
    "    review = open(file_name).read()\n",
    "    allow = string.ascii_letters + string.whitespace + \"'-\"\n",
    "    review = re.sub('[^%s]' % allow, '', review).lower().split()\n",
    "    \n",
    "    word_count = {}\n",
    "    \n",
    "    for word in review:\n",
    "        count = review.count(word)\n",
    "        word_count[word] = count\n",
    "        \n",
    "    converted_word_count = {}\n",
    "    \n",
    "    for word, count in word_count.items():\n",
    "        try:\n",
    "            index = vocab.index(word)\n",
    "            converted_word_count[index] = count\n",
    "        except ValueError:    \n",
    "            continue\n",
    "            \n",
    "    indices = list(converted_word_count.keys())\n",
    "    indices.sort()\n",
    "    \n",
    "    sorted_bow = {}\n",
    "    \n",
    "    for index in indices:\n",
    "        sorted_bow[index] = converted_word_count[index]\n",
    "        \n",
    "    return_string = \"0 \"\n",
    "    \n",
    "    for index, count in sorted_bow.items():\n",
    "        return_string += str(index)\n",
    "        return_string += \":\"\n",
    "        return_string += str(count)\n",
    "        return_string += \" \"\n",
    "    \n",
    "    return return_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 0:9 1:1 2:4 3:4 4:6 5:4 6:2 7:2 8:4 10:4 12:2 26:1 28:1 29:2 32:1 41:1 45:1 47:1 50:1 54:2 57:1 59:1 63:2 64:1 66:1 68:2 70:1 72:1 78:1 100:1 106:1 116:1 122:1 125:1 136:1 140:1 142:1 150:1 167:1 183:1 201:1 207:1 208:1 213:1 217:1 230:1 255:1 321:4 343:1 357:1 370:1 390:2 468:1 514:1 571:1 619:1 671:1 766:1 877:1 1057:1 1179:1 1192:1 1402:2 1416:1 1477:2 1940:1 1941:1 2096:1 2243:1 2285:1 2379:1 2934:1 2938:1 3520:1 3647:1 4938:1 5138:3 5715:1 5726:1 5731:1 5812:1 8319:1 8567:1 10480:1 14239:1 20604:1 22409:4 24551:1 47304:1 '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenizer(\"0_9.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
